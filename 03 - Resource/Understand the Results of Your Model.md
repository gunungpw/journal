# Understand the Results of Your Model

The first thing you'll do during the interpret stage is to try and answer some basic questions. Some questions to ask yourself in the interpret stage include, what was the objective of this analysis? It's important to go back to your starting point because that will remind you of the questions you set out to answer. It's quite easy to get lost in the data during the model and explore stages and lose sight of your initial question. Then ask yourself, how does the data answer my questions? Maybe the data shows you that the business goal you set is currently unattainable, or maybe it gives you a plan that you could use to move forward. Another question to ask is, what other learnings do I have? In the process of answering one business question, you will often find new pieces of potentially useful information that help solve the problem at hand in a different way. Or maybe they open up new potential business objectives you can address in later analysis. How can I apply this to a business context? Gaining new knowledge is great, but it is important to focus on information that's actionable and moves your business forward in meaningful ways. It will often be someone else that takes action based on your information, so think about how that will happen. Perhaps the most important question to ask is, how confident should I be in my results? If you see an improvement in a business metric, was it due to the changes you made or was it due to random chance? Many data analysts are overconfident in their results, and when they implement what they have learned, they quickly discover that something was wrong with their analysis. That brings us to the topic of, how do you know if you should be confident in the results of your model? Earlier, during the modeling stage, we briefly discussed using a separate set of test data to check your trained model against. The testing process is all about ensuring you have the right amount of confidence in your model. By running your test data through your model, you can answer questions like, how wrong is the model on average? If the model predicts something, how likely is it to be correct or incorrect? Are there particular scenarios that cause the model to be incorrect? Even the best models have limitations, so it's important to know what they are. On top of those basic questions, you can also use a tool called statistical testing to quantify how confident you should be. Statistical tests are mathematical methods of ensuring that differences are not caused by random chance. Sometimes this is called the significance of the results. For example, you're trying to improve an email campaign. Your model recommends a change that you implement, and you see a 5% increase in sales. Great, you just made the company 5% more money, right? Well, not so fast, it's possible that the increase in sales was random or because of some other factor. How can you know? You might run a statistical test and see that you should be 80% confident that the change in revenue was due to your new improved emails. It's then up to you or your organization to decide how confident you need to be to take action. Sometimes organizations want to be between 90 and 95% confident to take action, other times, they're happy with greater than 50%. It often depends on how risky it is for your business to be wrong. This is why statistical tests are so useful to businesses. So how do statistical tests work? To be honest, there's a lot of complicated math involved that we won't get into here. But there are some things they generally measure, including the differences in the averages of the datasets. If the averages are very different, the difference is less likely to be caused by randomness, the size of the dataset. The more data you have, the more confident you should be that the difference in averages isn't random, even if it's small. And the distributions of the datasets, this is often measured using standard deviation. A high standard deviation indicates high variability or that data values on average fall far from the mean. And a low standard deviation would mean that data in general is closer to the mean. If your data sets have high standard deviations, even large differences in their averages can simply be due to randomness. It's important to note here that none of these metrics in isolation provide a quantifiable measure of confidence. Only by combining them using statistical tests can you measure confidence.

The interpret phase of the awesome framework is crucial because it's where data driven insights are evaluated and communicated. You must revisit your initial analysis objectives, understand how the data answers your questions, and uncover any additional findings. Moreover, it's vital to ensure these insights are actionable within your business context. Tools like statistical testing play a key role, helping quantify your confidence in the results. And ultimately, the goal of the interpret phase and data analysis overall is not just to gain insights, but to make informed decisions that propel your business forward.
â€‹
